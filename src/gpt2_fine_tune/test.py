import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from utils.cuda_data import Cuda_data

import config

class LLM:
    tokenizer = None
    model = None

    response_parser = None

    def __init__(self, response_parser=None):
        self.cuda = Cuda_data()
        self.cuda.check()
        self.load_model()

        self.response_parser = response_parser or self.default_response_parser


    def load_model(self):
        print("–ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å...")
        self.tokenizer = AutoTokenizer.from_pretrained(config.MODEL_PATH)
        self.model = AutoModelForCausalLM.from_pretrained(config.MODEL_PATH)

        # –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–æ: –∏—Å–ø–æ–ª—å–∑—É–µ–º self.cuda.device –∫–∞–∫ –æ–±—ä–µ–∫—Ç, –∞ –Ω–µ —Ñ—É–Ω–∫—Ü–∏—é
        self.model.to(self.cuda.device)
        self.model.eval()
        print("–ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞!")

    # =========================
    # –ü–∞—Ä—Å–µ—Ä –¥–ª—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–æ–º–ø—Ç–∞.
    # –ó–∞–≤–∏—Å–∏—Ç –æ—Ç –¥–∞—Ç–∞—Å–µ—Ç–∞: –≤ –º–æ–µ–º —Å–ª—É—á–∞–µ –ø–æ–ª–∏—Ä–æ–≤–∫–∞ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏–ª–∞
    # –ø–æ—Å—Ä–µ–¥—Å—Ç–≤–æ–º —Ç–µ–≥–æ–≤ assistant –∏ user –±–µ–∑ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞.
    # –ü—Ä–∏ –∂–µ–ª–∞–Ω–∏–∏, –º–æ–∂–Ω–æ –∏–∑–º–µ–Ω–∏—Ç—å —ç—Ç–æ—Ç –º–µ—Ç–æ–¥ –ø–æ–¥ —Å–≤–æ–∏ –Ω—É–∂–¥—ã.
    # =========================
    def default_response_parser(self, text, prompt):
        # –ë–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ –æ—Ç–≤–µ—Ç –ø–æ—Å–ª–µ –ø—Ä–æ–º–ø—Ç–∞
        response = text[len(prompt):]

        # –£–¥–∞–ª—è–µ–º "user:" –∏ "assistant:" (–ª—é–±–æ–π —Ä–µ–≥–∏—Å—Ç—Ä)
        import re
        response = re.sub(r'(?i)(user:|assistant:)', '', response)

        # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã –≤ –Ω–∞—á–∞–ª–µ
        response = response.lstrip()

        # –£–¥–∞–ª—è–µ–º –∑–∞–ø—è—Ç—É—é, —Ç–∏—Ä–µ –∏–ª–∏ –¥–≤–æ–µ—Ç–æ—á–∏–µ –≤ –Ω–∞—á–∞–ª–µ —Å—Ç—Ä–æ–∫–∏
        if response and response[0] in ',:-‚Äî':
            response = response[1:].lstrip()

        # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã –∏ –ø–µ—Ä–µ–Ω–æ—Å—ã
        response = response.strip()

        # –£–±–∏—Ä–∞–µ–º –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø–µ—Ä–µ–Ω–æ—Å—ã
        while '\n\n' in response:
            response = response.replace('\n\n', '\n')

        return response

    # =========================
    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞
    # =========================
    def generate_answer(self, prompt):
        inputs = self.tokenizer(prompt, return_tensors="pt").to(self.cuda.device)

        with torch.no_grad():
            output_ids = self.model.generate(
                **inputs,
                max_new_tokens=config.MAX_NEW_TOKENS,
                do_sample=True,
                temperature=0.7,
                top_p=0.8,
                repetition_penalty=1.1,
                eos_token_id=self.tokenizer.eos_token_id,
            )

        text = self.tokenizer.decode(output_ids[0], skip_special_tokens=True)

        # –ë–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ –æ—Ç–≤–µ—Ç –ø–æ—Å–ª–µ –ø—Ä–æ–º–ø—Ç–∞
        response = self.response_parser(text, prompt)

        # –ï—Å–ª–∏ –æ—Ç–≤–µ—Ç –ø—É—Å—Ç–æ–π
        if not response:
            return "ü§î"

        return response